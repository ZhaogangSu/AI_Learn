{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Positional Encoding\n",
    "class SinusoidalPE(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=5000) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -torch.log(torch.tensor(10000.0))\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :] # type: ignore\n",
    "        \n",
    "\n",
    "# Multi-head Attention\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, d_model, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        assert d_model == self.d_model\n",
    "\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = Q @ K.transpose(-1, -2)\n",
    "        attn_weight = torch.softmax(scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        output = (attn_weight @ V).transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Feed Forward\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(nn.functional.gelu(self.fc1(x)))\n",
    "    \n",
    "# Transformer Block (LayerNorm + MHA + Res + LayerNorm + Fnn + Res)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attn = MultiHeadAttn(d_model, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.fnn = FeedForward(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.multi_head_attn(self.norm1(x)))\n",
    "        x = x + self.dropout(self.fnn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Transformer Encoder (repeated Transformer Block)\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, num_layers, max_seq_len=5000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPE(d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, n_heads, d_ff) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "max_seq_len = 5000\n",
    "\n",
    "head_dim = d_model // n_heads\n",
    "\n",
    "model = TransformerEncoder(vocab_size, d_model, n_heads, d_ff, num_layers, max_seq_len)\n",
    "\n",
    "token_ids = torch.randint(0, vocab_size, (2, 10, 512))\n",
    "print(token_ids.shape)\n",
    "# output = model(token_ids)\n",
    "\n",
    "# print(output.shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4mip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
